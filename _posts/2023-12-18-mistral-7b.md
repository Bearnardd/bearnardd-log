---
layout: post
title: Small, Efficient and Powerful LLM - Mistral 7b
tag: llms, open-source, nlp, mistral
date: 2023-12-04 11:10:05
desc: Deep dive into implementation details of Mistral 7b LLM 
author: Bearnardd
---

## Introduction

Most people have either used or at least heard about ChatGPT, GPT-4, and Bard of Claude. One of the connections between their traits is all of them are accessible only via closed API (blackbox) or GUI. However, it's essential to note that significant work has also been undertaken within the open-source landscape of LLM. This blog post delves into exploring one such model, namely **Mistral 7b**, and describes its key characteristics.

## Mistral 7b

[Paper](https://arxiv.org/pdf/2310.06825.pdf)   
[Official implementation](https://github.com/mistralai/mistral-src/tree/main/)

**Mistral 7b** is a 7b parameter model released by **Mistral.ai** team. It has a transformer-based architecture with the following implementation choices:
 - Grouped-Query Attention
 - Sliding Window Attention
 - Rolling Buffer Cache
 - Pre-fill and chunking

 **#### Grouped-Query Attention**

 * significantly accelerates the inference speed
 * reduces memory requirements during decoding, facilitating higher batch sizes and subsequently, higher throughput

 **Grouped-query attention** is a generalization of **Multi-query attention**, utilizing a single key-value head for all query heads.
It seeks to reduce computational costs and memory usage while maintaining the overall performance of **Multi-head attention**. GQA strikes a balance between using distinct kv heads for each single head (like in MHA) and the MQA approach,
where a single kv head is used. It achieves this by using a single kv head per subgroup of query heads.

![]({{"/assets/images/MHA.png" | relative_url }} )
![]({{"/assets/images/MQA.png" | relative_url }} )
![]({{"/assets/images/GQA.png" | relative_url }} )

**GQA-G** refers grouped-query attention with G groups. Using GQA, we can create both MQA (with G=1) and MHA (with G=H where H represents the number of attention heads). Additionally, it's possible to transform an MH checkpoint into a GQA checkpoint by creating each group through the mean pooling of all the original heads within that group


**#### Sliding Window Attention**

* allows handling longer sequences more effectively at a reduced computational cost
* at inference time, this incurs lower latency and higher throughput due to increased cache availability

The main drawback of **self attention** operation is its computational complexity, which is quadratic
w.r.t to the input sequence length (since each token in the sequence attends to every other token in the sequence leading to $n \times n$ attention matrix that involves matrix multiplication resulting in $O({n^2 \times d})$ time complexity, where $n$ denotes the length of the input sequence and $d$ denotes the dimensionality of the embeddings) and it's memory complexity which is linear w.r.t to the input sequence length

Sliding window attention tries to alleviate this issue by allowing each token in the input sequence to attend to at most **W** tokens from the previous layer. It is important to understand that even though the local window for each token is small it does not mean
that the tokens outside the window do not influence the next-word prediction since there is a stack of attention layers and at each layer, information moves forward by **W** tokens. So after **k** attention layers information moves by $k \times W$ tokens.


![]({{"/assets/images/ClassicalAttention.png" | relative_url }} )
![]({{"/assets/images/SlidingWindowAttention.png" | relative_url }} )
![]({{"/assets/images/swa_information_flow.png" | relative_url }} )


**#### Rolling Buffer Cache**

Rolling buffer cache is a type of data storage that keeps a fixed-size window of recently visited data. When at full capacity, the oldest data is replaced by the newly accessed data. Since sliding window attention has a fixed-size attention window we
can easily limit cache size by using a rolling buffer cache with a buffer size of **W**. The overwriting mechanism can be implemented by modulo operation. Both keys and values for timestamp **i** are stored inside the cache at the position $i mod W$. In
the result when $i$ exceeds buffer size it overwrites the oldest entries in a buffer. 

![]({{"/assets/images/rolling_buffer_cache.png" | relative_url }} )


**#### Pre-fill and chunking**

Even though transformer-based models generate sequences by predicting the next tokens in a one-by-one fashion we can increase the inference speed by pre-filling the key-value cache with the prompt which is known in advance. If we are dealing with a very
large prompt we can chunk it into smaller pieces and pre-fill the cache with each chunk separately (select the window size as chunk size). Then, for each chunk, we have to compute attention over it and the cache.


## Conclusions

In this blog post, we described key implementation details of Mistral 7b LLM. It is worth to mention that **Mistral** team recently released a new model **Mixtral 8x7b** which is a sparse mixture of experts models (SMoE). I will add a blog
post about its implementation as soon as it is out for the public (if it will be :).